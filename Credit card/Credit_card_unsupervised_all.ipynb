{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Credit card unsupervised all.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPI5NbtW9/RyygG5Vqz9bdT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duybluemind1988/Data-science/blob/master/Hands%20on%20Unsupervised%20Oreilly/Credit_card_unsupervised_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swv_E86mrR40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98810eb-6b70-46fb-b2da-99fe8beb916b"
      },
      "source": [
        "# Load basic library\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tensorflow import random\n",
        "from sklearn.externals import joblib \n",
        "from scipy import stats\n",
        "import itertools\n",
        "\n",
        "# Load Keras\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import regularizers\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.regularizers import l1\n",
        "from keras.regularizers import l2\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dropout\n",
        "\n",
        "#Load sklearn\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn.metrics import log_loss \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score, precision_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import f1_score,balanced_accuracy_score\n",
        "from sklearn.metrics import average_precision_score \n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA, NMF\n",
        "\n",
        "#ML normal ML\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "#ML ensembles\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "#Special\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "#Load visual neural:\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "\n",
        "# import require packages for plotting\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.express as px\n",
        "import altair as alt # no need to add altviewer\n",
        "from pandas.plotting import scatter_matrix\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUgROMNlrm8I"
      },
      "source": [
        "# Dimension reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW-XjyyGq-c-"
      },
      "source": [
        "file='https://media.githubusercontent.com/media/aapatel09/handson-unsupervised-learning/master/datasets/credit_card_data/credit_card.csv'\n",
        "data = pd.read_csv(file)\n",
        "\n",
        "dataX = data.copy().drop(['Class'],axis=1)\n",
        "dataY = data['Class'].copy()\n",
        "\n",
        "featuresToScale = dataX.columns\n",
        "sX =StandardScaler(copy=True)\n",
        "dataX.loc[:,featuresToScale] = sX.fit_transform(dataX[featuresToScale])\n",
        "\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(dataX, dataY, test_size=0.20, \\\n",
        "                    random_state=2018, stratify=dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "0KqdYBkhreFk",
        "outputId": "2d68cf23-2cd8-4c2b-8323-00b98189082d"
      },
      "source": [
        "print(X_train.shape)\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(227845, 30)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>212834</th>\n",
              "      <td>0.930700</td>\n",
              "      <td>-0.193563</td>\n",
              "      <td>-0.136547</td>\n",
              "      <td>-0.044610</td>\n",
              "      <td>-1.471851</td>\n",
              "      <td>0.299420</td>\n",
              "      <td>-0.447038</td>\n",
              "      <td>0.065017</td>\n",
              "      <td>-0.903609</td>\n",
              "      <td>-0.798323</td>\n",
              "      <td>0.777325</td>\n",
              "      <td>0.567004</td>\n",
              "      <td>-1.292424</td>\n",
              "      <td>-1.835557</td>\n",
              "      <td>0.267809</td>\n",
              "      <td>-0.676375</td>\n",
              "      <td>1.297130</td>\n",
              "      <td>-0.277600</td>\n",
              "      <td>-0.458910</td>\n",
              "      <td>0.618104</td>\n",
              "      <td>-0.431244</td>\n",
              "      <td>1.781078</td>\n",
              "      <td>1.908755</td>\n",
              "      <td>-0.224010</td>\n",
              "      <td>1.282179</td>\n",
              "      <td>-1.551742</td>\n",
              "      <td>-0.388800</td>\n",
              "      <td>0.018078</td>\n",
              "      <td>0.019746</td>\n",
              "      <td>-0.189908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175643</th>\n",
              "      <td>0.581475</td>\n",
              "      <td>1.145707</td>\n",
              "      <td>-0.464983</td>\n",
              "      <td>-1.365275</td>\n",
              "      <td>-0.886248</td>\n",
              "      <td>0.305657</td>\n",
              "      <td>0.066829</td>\n",
              "      <td>-0.273461</td>\n",
              "      <td>-0.153975</td>\n",
              "      <td>-0.720206</td>\n",
              "      <td>0.881148</td>\n",
              "      <td>0.098351</td>\n",
              "      <td>0.322958</td>\n",
              "      <td>1.307987</td>\n",
              "      <td>-0.171925</td>\n",
              "      <td>-1.092521</td>\n",
              "      <td>1.037590</td>\n",
              "      <td>-0.323926</td>\n",
              "      <td>-0.816001</td>\n",
              "      <td>1.463272</td>\n",
              "      <td>0.145879</td>\n",
              "      <td>0.755524</td>\n",
              "      <td>2.356802</td>\n",
              "      <td>-0.473290</td>\n",
              "      <td>-0.455927</td>\n",
              "      <td>1.229323</td>\n",
              "      <td>0.632015</td>\n",
              "      <td>-0.113455</td>\n",
              "      <td>-0.257673</td>\n",
              "      <td>-0.253277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32164</th>\n",
              "      <td>-1.224452</td>\n",
              "      <td>-0.132705</td>\n",
              "      <td>0.713528</td>\n",
              "      <td>0.633126</td>\n",
              "      <td>0.630864</td>\n",
              "      <td>0.100135</td>\n",
              "      <td>0.231616</td>\n",
              "      <td>0.130995</td>\n",
              "      <td>-1.418178</td>\n",
              "      <td>-0.417453</td>\n",
              "      <td>0.428285</td>\n",
              "      <td>1.664365</td>\n",
              "      <td>1.082977</td>\n",
              "      <td>0.337247</td>\n",
              "      <td>0.279745</td>\n",
              "      <td>0.476292</td>\n",
              "      <td>-0.411223</td>\n",
              "      <td>-0.270911</td>\n",
              "      <td>-0.000925</td>\n",
              "      <td>0.079491</td>\n",
              "      <td>-0.446114</td>\n",
              "      <td>2.304290</td>\n",
              "      <td>0.332887</td>\n",
              "      <td>0.582661</td>\n",
              "      <td>0.332978</td>\n",
              "      <td>-2.285405</td>\n",
              "      <td>-1.196946</td>\n",
              "      <td>0.395602</td>\n",
              "      <td>-0.122872</td>\n",
              "      <td>-0.273268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79783</th>\n",
              "      <td>-0.771538</td>\n",
              "      <td>0.677347</td>\n",
              "      <td>-0.593270</td>\n",
              "      <td>0.134047</td>\n",
              "      <td>-0.819728</td>\n",
              "      <td>-0.374058</td>\n",
              "      <td>0.793413</td>\n",
              "      <td>-0.989350</td>\n",
              "      <td>0.295608</td>\n",
              "      <td>-0.421328</td>\n",
              "      <td>0.672262</td>\n",
              "      <td>-0.039082</td>\n",
              "      <td>-0.728508</td>\n",
              "      <td>-0.042104</td>\n",
              "      <td>-0.215013</td>\n",
              "      <td>0.938789</td>\n",
              "      <td>2.189987</td>\n",
              "      <td>-0.602120</td>\n",
              "      <td>-0.197767</td>\n",
              "      <td>0.977359</td>\n",
              "      <td>0.230517</td>\n",
              "      <td>0.499247</td>\n",
              "      <td>1.235857</td>\n",
              "      <td>-0.528168</td>\n",
              "      <td>-2.753810</td>\n",
              "      <td>0.998054</td>\n",
              "      <td>0.138978</td>\n",
              "      <td>0.089492</td>\n",
              "      <td>0.003362</td>\n",
              "      <td>-0.133535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107026</th>\n",
              "      <td>-0.517390</td>\n",
              "      <td>-0.161335</td>\n",
              "      <td>0.702321</td>\n",
              "      <td>0.761409</td>\n",
              "      <td>-0.019016</td>\n",
              "      <td>0.180568</td>\n",
              "      <td>-0.574303</td>\n",
              "      <td>0.599706</td>\n",
              "      <td>-0.067450</td>\n",
              "      <td>-0.410422</td>\n",
              "      <td>-0.483954</td>\n",
              "      <td>-0.433314</td>\n",
              "      <td>0.084132</td>\n",
              "      <td>0.714155</td>\n",
              "      <td>-0.580312</td>\n",
              "      <td>0.941704</td>\n",
              "      <td>0.527760</td>\n",
              "      <td>-0.179441</td>\n",
              "      <td>-0.103065</td>\n",
              "      <td>0.049886</td>\n",
              "      <td>0.207703</td>\n",
              "      <td>-0.382678</td>\n",
              "      <td>-0.981056</td>\n",
              "      <td>-0.103749</td>\n",
              "      <td>-0.019104</td>\n",
              "      <td>-0.181466</td>\n",
              "      <td>0.191556</td>\n",
              "      <td>0.615710</td>\n",
              "      <td>0.290262</td>\n",
              "      <td>-0.338876</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Time        V1        V2  ...       V27       V28    Amount\n",
              "212834  0.930700 -0.193563 -0.136547  ...  0.018078  0.019746 -0.189908\n",
              "175643  0.581475  1.145707 -0.464983  ... -0.113455 -0.257673 -0.253277\n",
              "32164  -1.224452 -0.132705  0.713528  ...  0.395602 -0.122872 -0.273268\n",
              "79783  -0.771538  0.677347 -0.593270  ...  0.089492  0.003362 -0.133535\n",
              "107026 -0.517390 -0.161335  0.702321  ...  0.615710  0.290262 -0.338876\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAPQswvvrfgX",
        "outputId": "df1df2c2-ce30-47bb-a65f-afe21cd38a5f"
      },
      "source": [
        "y_train.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.998271\n",
              "1    0.001729\n",
              "Name: Class, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeX0Rsb8rfd6"
      },
      "source": [
        "def anomalyScores(originalDF, reducedDF):\n",
        "    loss = np.sum((np.array(originalDF)-np.array(reducedDF))**2, axis=1)\n",
        "    loss = pd.Series(data=loss,index=originalDF.index)\n",
        "    loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSdohuqonQ1e"
      },
      "source": [
        "def scatterPlot(xDF, yDF, algoName):\n",
        "    tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)\n",
        "    tempDF = pd.concat((tempDF,yDF), axis=1, join=\"inner\")\n",
        "    tempDF.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n",
        "    sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", \\\n",
        "               data=tempDF, fit_reg=False)\n",
        "    ax = plt.gca()\n",
        "    ax.set_title(\"Separation of Observations using \"+algoName)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzShsDviz4W7"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "2itf4wtXOwJK",
        "outputId": "01341730-45c1-4fb8-98a9-649c218c8cbf"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "whiten = False\n",
        "random_state = 2018\n",
        "\n",
        "pca = PCA(0.95, whiten=whiten, \\\n",
        "          random_state=random_state)\n",
        "\n",
        "X_train_PCA = pca.fit_transform(X_train)\n",
        "X_train_PCA = pd.DataFrame(data=X_train_PCA, index=X_train.index)\n",
        "X_train_PCA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>212834</th>\n",
              "      <td>-0.328386</td>\n",
              "      <td>-1.647120</td>\n",
              "      <td>-0.370647</td>\n",
              "      <td>1.069640</td>\n",
              "      <td>-0.448299</td>\n",
              "      <td>0.033146</td>\n",
              "      <td>0.104970</td>\n",
              "      <td>-0.257753</td>\n",
              "      <td>-0.144465</td>\n",
              "      <td>0.138775</td>\n",
              "      <td>-0.183408</td>\n",
              "      <td>-0.699255</td>\n",
              "      <td>2.408763</td>\n",
              "      <td>-0.276889</td>\n",
              "      <td>-0.647064</td>\n",
              "      <td>1.839798</td>\n",
              "      <td>-0.454629</td>\n",
              "      <td>1.126125</td>\n",
              "      <td>1.077458</td>\n",
              "      <td>-0.547043</td>\n",
              "      <td>-0.120453</td>\n",
              "      <td>-0.698270</td>\n",
              "      <td>-1.131134</td>\n",
              "      <td>1.133642</td>\n",
              "      <td>-2.028178</td>\n",
              "      <td>0.259533</td>\n",
              "      <td>0.249779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175643</th>\n",
              "      <td>-0.391033</td>\n",
              "      <td>-1.499358</td>\n",
              "      <td>-0.174836</td>\n",
              "      <td>-0.021138</td>\n",
              "      <td>-0.126573</td>\n",
              "      <td>-0.259141</td>\n",
              "      <td>0.018763</td>\n",
              "      <td>-0.119671</td>\n",
              "      <td>0.495395</td>\n",
              "      <td>-0.388307</td>\n",
              "      <td>-0.624634</td>\n",
              "      <td>-0.320947</td>\n",
              "      <td>-0.371824</td>\n",
              "      <td>-0.592912</td>\n",
              "      <td>-1.886222</td>\n",
              "      <td>-0.120720</td>\n",
              "      <td>-2.147871</td>\n",
              "      <td>0.932113</td>\n",
              "      <td>1.861459</td>\n",
              "      <td>1.074669</td>\n",
              "      <td>1.646203</td>\n",
              "      <td>0.353684</td>\n",
              "      <td>-0.098525</td>\n",
              "      <td>0.626862</td>\n",
              "      <td>-0.000636</td>\n",
              "      <td>-0.441325</td>\n",
              "      <td>-0.427625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32164</th>\n",
              "      <td>-0.251329</td>\n",
              "      <td>0.768326</td>\n",
              "      <td>-0.715681</td>\n",
              "      <td>2.481347</td>\n",
              "      <td>-0.233165</td>\n",
              "      <td>-0.264697</td>\n",
              "      <td>-0.017123</td>\n",
              "      <td>0.073743</td>\n",
              "      <td>0.015488</td>\n",
              "      <td>0.233733</td>\n",
              "      <td>0.703183</td>\n",
              "      <td>-0.071201</td>\n",
              "      <td>0.343467</td>\n",
              "      <td>0.700585</td>\n",
              "      <td>-0.097981</td>\n",
              "      <td>0.033051</td>\n",
              "      <td>0.043164</td>\n",
              "      <td>-0.303798</td>\n",
              "      <td>-0.778685</td>\n",
              "      <td>-0.165679</td>\n",
              "      <td>-1.181274</td>\n",
              "      <td>0.036525</td>\n",
              "      <td>-1.240614</td>\n",
              "      <td>3.033557</td>\n",
              "      <td>-0.278349</td>\n",
              "      <td>-0.317095</td>\n",
              "      <td>0.429228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79783</th>\n",
              "      <td>-0.178978</td>\n",
              "      <td>0.806585</td>\n",
              "      <td>-0.015431</td>\n",
              "      <td>0.030605</td>\n",
              "      <td>-0.016860</td>\n",
              "      <td>-0.165207</td>\n",
              "      <td>0.066215</td>\n",
              "      <td>0.066120</td>\n",
              "      <td>0.317023</td>\n",
              "      <td>0.646959</td>\n",
              "      <td>-0.904334</td>\n",
              "      <td>-0.225339</td>\n",
              "      <td>-0.547126</td>\n",
              "      <td>-0.461122</td>\n",
              "      <td>-2.377110</td>\n",
              "      <td>-0.249525</td>\n",
              "      <td>0.098223</td>\n",
              "      <td>-0.746629</td>\n",
              "      <td>1.380486</td>\n",
              "      <td>1.547023</td>\n",
              "      <td>2.169981</td>\n",
              "      <td>-1.131580</td>\n",
              "      <td>-0.573755</td>\n",
              "      <td>-0.439980</td>\n",
              "      <td>-1.603572</td>\n",
              "      <td>-0.701797</td>\n",
              "      <td>0.178609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107026</th>\n",
              "      <td>-0.422434</td>\n",
              "      <td>0.802797</td>\n",
              "      <td>0.092058</td>\n",
              "      <td>-0.132500</td>\n",
              "      <td>-0.083164</td>\n",
              "      <td>-0.063562</td>\n",
              "      <td>0.446350</td>\n",
              "      <td>0.376678</td>\n",
              "      <td>-0.042132</td>\n",
              "      <td>0.917738</td>\n",
              "      <td>0.002988</td>\n",
              "      <td>0.965396</td>\n",
              "      <td>-0.602810</td>\n",
              "      <td>0.467631</td>\n",
              "      <td>0.865096</td>\n",
              "      <td>-0.527295</td>\n",
              "      <td>0.193088</td>\n",
              "      <td>-0.278194</td>\n",
              "      <td>0.393471</td>\n",
              "      <td>-1.101177</td>\n",
              "      <td>0.360459</td>\n",
              "      <td>-0.519215</td>\n",
              "      <td>-0.087464</td>\n",
              "      <td>0.102168</td>\n",
              "      <td>0.138395</td>\n",
              "      <td>-0.142742</td>\n",
              "      <td>0.069431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48687</th>\n",
              "      <td>2.790033</td>\n",
              "      <td>0.951285</td>\n",
              "      <td>-0.008367</td>\n",
              "      <td>0.087185</td>\n",
              "      <td>-0.694909</td>\n",
              "      <td>0.506011</td>\n",
              "      <td>-0.289624</td>\n",
              "      <td>-0.168223</td>\n",
              "      <td>-0.008594</td>\n",
              "      <td>1.003334</td>\n",
              "      <td>0.630052</td>\n",
              "      <td>1.042305</td>\n",
              "      <td>-0.758453</td>\n",
              "      <td>-0.840284</td>\n",
              "      <td>0.844799</td>\n",
              "      <td>-0.917007</td>\n",
              "      <td>-0.214642</td>\n",
              "      <td>-0.268632</td>\n",
              "      <td>0.636924</td>\n",
              "      <td>-0.212475</td>\n",
              "      <td>0.862443</td>\n",
              "      <td>-0.164387</td>\n",
              "      <td>0.910430</td>\n",
              "      <td>-1.334788</td>\n",
              "      <td>-0.886176</td>\n",
              "      <td>-0.937503</td>\n",
              "      <td>0.616739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159608</th>\n",
              "      <td>-0.435838</td>\n",
              "      <td>-1.079428</td>\n",
              "      <td>0.057791</td>\n",
              "      <td>-0.084610</td>\n",
              "      <td>-0.140470</td>\n",
              "      <td>-0.219667</td>\n",
              "      <td>0.503158</td>\n",
              "      <td>0.309035</td>\n",
              "      <td>0.038082</td>\n",
              "      <td>1.216414</td>\n",
              "      <td>0.218779</td>\n",
              "      <td>0.290398</td>\n",
              "      <td>-0.000130</td>\n",
              "      <td>0.344989</td>\n",
              "      <td>0.028067</td>\n",
              "      <td>-1.451734</td>\n",
              "      <td>0.050735</td>\n",
              "      <td>-1.224011</td>\n",
              "      <td>0.576376</td>\n",
              "      <td>-0.877380</td>\n",
              "      <td>0.279461</td>\n",
              "      <td>-1.028943</td>\n",
              "      <td>1.163840</td>\n",
              "      <td>-1.006927</td>\n",
              "      <td>-0.226073</td>\n",
              "      <td>0.215674</td>\n",
              "      <td>0.323378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205176</th>\n",
              "      <td>8.201056</td>\n",
              "      <td>-1.394710</td>\n",
              "      <td>-0.372240</td>\n",
              "      <td>0.314482</td>\n",
              "      <td>-1.878672</td>\n",
              "      <td>1.714176</td>\n",
              "      <td>-0.723460</td>\n",
              "      <td>-1.789461</td>\n",
              "      <td>-1.012728</td>\n",
              "      <td>-0.128946</td>\n",
              "      <td>1.899642</td>\n",
              "      <td>0.385919</td>\n",
              "      <td>1.082216</td>\n",
              "      <td>-1.189724</td>\n",
              "      <td>1.105657</td>\n",
              "      <td>-0.545348</td>\n",
              "      <td>0.309816</td>\n",
              "      <td>-0.755390</td>\n",
              "      <td>0.497387</td>\n",
              "      <td>-0.994486</td>\n",
              "      <td>2.089284</td>\n",
              "      <td>-0.887795</td>\n",
              "      <td>-1.161978</td>\n",
              "      <td>-0.259862</td>\n",
              "      <td>-0.644684</td>\n",
              "      <td>0.879939</td>\n",
              "      <td>-1.040974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197673</th>\n",
              "      <td>-0.603686</td>\n",
              "      <td>-2.393943</td>\n",
              "      <td>0.522242</td>\n",
              "      <td>-0.515993</td>\n",
              "      <td>-0.521553</td>\n",
              "      <td>0.217642</td>\n",
              "      <td>0.899988</td>\n",
              "      <td>-0.766855</td>\n",
              "      <td>-0.021740</td>\n",
              "      <td>-1.683623</td>\n",
              "      <td>-0.617853</td>\n",
              "      <td>-3.180667</td>\n",
              "      <td>-1.347854</td>\n",
              "      <td>0.044459</td>\n",
              "      <td>0.318835</td>\n",
              "      <td>0.894758</td>\n",
              "      <td>0.864102</td>\n",
              "      <td>0.339722</td>\n",
              "      <td>0.022207</td>\n",
              "      <td>-2.080588</td>\n",
              "      <td>-2.503211</td>\n",
              "      <td>-0.536475</td>\n",
              "      <td>0.734151</td>\n",
              "      <td>-0.362354</td>\n",
              "      <td>0.254577</td>\n",
              "      <td>-1.083796</td>\n",
              "      <td>0.880067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151952</th>\n",
              "      <td>0.423819</td>\n",
              "      <td>0.767632</td>\n",
              "      <td>-0.267885</td>\n",
              "      <td>-0.222386</td>\n",
              "      <td>0.311018</td>\n",
              "      <td>-0.129255</td>\n",
              "      <td>0.911732</td>\n",
              "      <td>-0.659877</td>\n",
              "      <td>-0.845075</td>\n",
              "      <td>-1.074328</td>\n",
              "      <td>0.435796</td>\n",
              "      <td>-1.109688</td>\n",
              "      <td>2.549561</td>\n",
              "      <td>1.657972</td>\n",
              "      <td>0.255843</td>\n",
              "      <td>2.368642</td>\n",
              "      <td>-0.559299</td>\n",
              "      <td>-0.040419</td>\n",
              "      <td>2.019560</td>\n",
              "      <td>-0.232365</td>\n",
              "      <td>0.746700</td>\n",
              "      <td>-1.586883</td>\n",
              "      <td>-0.200942</td>\n",
              "      <td>-1.777225</td>\n",
              "      <td>-1.799383</td>\n",
              "      <td>0.039290</td>\n",
              "      <td>-0.730204</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>227845 rows Ã— 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         1         2   ...        24        25        26\n",
              "212834 -0.328386 -1.647120 -0.370647  ... -2.028178  0.259533  0.249779\n",
              "175643 -0.391033 -1.499358 -0.174836  ... -0.000636 -0.441325 -0.427625\n",
              "32164  -0.251329  0.768326 -0.715681  ... -0.278349 -0.317095  0.429228\n",
              "79783  -0.178978  0.806585 -0.015431  ... -1.603572 -0.701797  0.178609\n",
              "107026 -0.422434  0.802797  0.092058  ...  0.138395 -0.142742  0.069431\n",
              "...          ...       ...       ...  ...       ...       ...       ...\n",
              "48687   2.790033  0.951285 -0.008367  ... -0.886176 -0.937503  0.616739\n",
              "159608 -0.435838 -1.079428  0.057791  ... -0.226073  0.215674  0.323378\n",
              "205176  8.201056 -1.394710 -0.372240  ... -0.644684  0.879939 -1.040974\n",
              "197673 -0.603686 -2.393943  0.522242  ...  0.254577 -1.083796  0.880067\n",
              "151952  0.423819  0.767632 -0.267885  ... -1.799383  0.039290 -0.730204\n",
              "\n",
              "[227845 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "jMpaXSC3rzDE",
        "outputId": "bbebcac6-0a8f-4030-a1fe-a6393bbfdad8"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "n_components = 27\n",
        "whiten = False\n",
        "random_state = 2018\n",
        "\n",
        "pca = PCA(n_components=n_components, whiten=whiten, \\\n",
        "          random_state=random_state)\n",
        "\n",
        "X_train_PCA = pca.fit_transform(X_train)\n",
        "X_train_PCA = pd.DataFrame(data=X_train_PCA, index=X_train.index)\n",
        "\n",
        "X_train_PCA_inverse = pca.inverse_transform(X_train_PCA)\n",
        "X_train_PCA_inverse = pd.DataFrame(data=X_train_PCA_inverse, \\\n",
        "                                   index=X_train.index)\n",
        "\n",
        "scatterPlot(X_train_PCA, y_train, \"PCA\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 406.6x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "PJfxkvmcnSj2",
        "outputId": "688861c4-ad31-487e-fe99-784d66ea6bde"
      },
      "source": [
        "X_train_PCA_inverse.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>212834</th>\n",
              "      <td>1.158885</td>\n",
              "      <td>-0.230776</td>\n",
              "      <td>-0.131010</td>\n",
              "      <td>0.098186</td>\n",
              "      <td>-1.438465</td>\n",
              "      <td>0.245425</td>\n",
              "      <td>-0.427459</td>\n",
              "      <td>0.036078</td>\n",
              "      <td>-0.891368</td>\n",
              "      <td>-0.794555</td>\n",
              "      <td>0.769308</td>\n",
              "      <td>0.648595</td>\n",
              "      <td>-1.331473</td>\n",
              "      <td>-1.813925</td>\n",
              "      <td>0.301849</td>\n",
              "      <td>-0.614941</td>\n",
              "      <td>1.295263</td>\n",
              "      <td>-0.250496</td>\n",
              "      <td>-0.488517</td>\n",
              "      <td>0.607830</td>\n",
              "      <td>-0.416024</td>\n",
              "      <td>1.765854</td>\n",
              "      <td>1.860641</td>\n",
              "      <td>-0.240314</td>\n",
              "      <td>1.287519</td>\n",
              "      <td>-1.473860</td>\n",
              "      <td>-0.375029</td>\n",
              "      <td>0.019946</td>\n",
              "      <td>0.022692</td>\n",
              "      <td>-0.184939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175643</th>\n",
              "      <td>1.068744</td>\n",
              "      <td>1.064956</td>\n",
              "      <td>-0.451318</td>\n",
              "      <td>-1.063156</td>\n",
              "      <td>-0.813092</td>\n",
              "      <td>0.189380</td>\n",
              "      <td>0.107616</td>\n",
              "      <td>-0.339394</td>\n",
              "      <td>-0.127267</td>\n",
              "      <td>-0.713247</td>\n",
              "      <td>0.860652</td>\n",
              "      <td>0.274696</td>\n",
              "      <td>0.235324</td>\n",
              "      <td>1.354569</td>\n",
              "      <td>-0.102706</td>\n",
              "      <td>-0.961628</td>\n",
              "      <td>1.030428</td>\n",
              "      <td>-0.271244</td>\n",
              "      <td>-0.881205</td>\n",
              "      <td>1.442165</td>\n",
              "      <td>0.177689</td>\n",
              "      <td>0.722138</td>\n",
              "      <td>2.254794</td>\n",
              "      <td>-0.508642</td>\n",
              "      <td>-0.444527</td>\n",
              "      <td>1.394902</td>\n",
              "      <td>0.661439</td>\n",
              "      <td>-0.110189</td>\n",
              "      <td>-0.251929</td>\n",
              "      <td>-0.240983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32164</th>\n",
              "      <td>-0.532814</td>\n",
              "      <td>-0.252987</td>\n",
              "      <td>0.781670</td>\n",
              "      <td>1.036085</td>\n",
              "      <td>0.753874</td>\n",
              "      <td>-0.058894</td>\n",
              "      <td>0.266116</td>\n",
              "      <td>-0.034465</td>\n",
              "      <td>-1.368136</td>\n",
              "      <td>-0.419351</td>\n",
              "      <td>0.360914</td>\n",
              "      <td>1.941669</td>\n",
              "      <td>0.904731</td>\n",
              "      <td>0.408079</td>\n",
              "      <td>0.332013</td>\n",
              "      <td>0.658462</td>\n",
              "      <td>-0.461756</td>\n",
              "      <td>-0.262901</td>\n",
              "      <td>-0.120484</td>\n",
              "      <td>0.062839</td>\n",
              "      <td>-0.425895</td>\n",
              "      <td>2.240652</td>\n",
              "      <td>0.200577</td>\n",
              "      <td>0.531022</td>\n",
              "      <td>0.348846</td>\n",
              "      <td>-2.057416</td>\n",
              "      <td>-1.154801</td>\n",
              "      <td>0.389624</td>\n",
              "      <td>-0.122085</td>\n",
              "      <td>-0.188789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79783</th>\n",
              "      <td>-0.567660</td>\n",
              "      <td>0.622695</td>\n",
              "      <td>-0.581308</td>\n",
              "      <td>0.222736</td>\n",
              "      <td>-0.765269</td>\n",
              "      <td>-0.444535</td>\n",
              "      <td>0.804890</td>\n",
              "      <td>-1.052869</td>\n",
              "      <td>0.310377</td>\n",
              "      <td>-0.431990</td>\n",
              "      <td>0.622520</td>\n",
              "      <td>0.058719</td>\n",
              "      <td>-0.813657</td>\n",
              "      <td>-0.018054</td>\n",
              "      <td>-0.224515</td>\n",
              "      <td>0.990144</td>\n",
              "      <td>2.150828</td>\n",
              "      <td>-0.638863</td>\n",
              "      <td>-0.246558</td>\n",
              "      <td>0.976422</td>\n",
              "      <td>0.245328</td>\n",
              "      <td>0.478248</td>\n",
              "      <td>1.199715</td>\n",
              "      <td>-0.552183</td>\n",
              "      <td>-2.748957</td>\n",
              "      <td>1.057706</td>\n",
              "      <td>0.151385</td>\n",
              "      <td>0.083475</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.135757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107026</th>\n",
              "      <td>-0.562482</td>\n",
              "      <td>-0.172608</td>\n",
              "      <td>0.729390</td>\n",
              "      <td>0.692010</td>\n",
              "      <td>0.001775</td>\n",
              "      <td>0.177487</td>\n",
              "      <td>-0.593665</td>\n",
              "      <td>0.543523</td>\n",
              "      <td>-0.061304</td>\n",
              "      <td>-0.427170</td>\n",
              "      <td>-0.532133</td>\n",
              "      <td>-0.418166</td>\n",
              "      <td>0.029029</td>\n",
              "      <td>0.715619</td>\n",
              "      <td>-0.638440</td>\n",
              "      <td>0.925223</td>\n",
              "      <td>0.481178</td>\n",
              "      <td>-0.261672</td>\n",
              "      <td>-0.126579</td>\n",
              "      <td>0.064325</td>\n",
              "      <td>0.193871</td>\n",
              "      <td>-0.392830</td>\n",
              "      <td>-0.960570</td>\n",
              "      <td>-0.108260</td>\n",
              "      <td>-0.020246</td>\n",
              "      <td>-0.207576</td>\n",
              "      <td>0.189090</td>\n",
              "      <td>0.604616</td>\n",
              "      <td>0.281583</td>\n",
              "      <td>-0.313295</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         1         2   ...        27        28        29\n",
              "212834  1.158885 -0.230776 -0.131010  ...  0.019946  0.022692 -0.184939\n",
              "175643  1.068744  1.064956 -0.451318  ... -0.110189 -0.251929 -0.240983\n",
              "32164  -0.532814 -0.252987  0.781670  ...  0.389624 -0.122085 -0.188789\n",
              "79783  -0.567660  0.622695 -0.581308  ...  0.083475 -0.000217 -0.135757\n",
              "107026 -0.562482 -0.172608  0.729390  ...  0.604616  0.281583 -0.313295\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "L1LWqnCenU0E",
        "outputId": "5a04a133-c4d2-494f-be93-050b1565668a"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>212834</th>\n",
              "      <td>0.930700</td>\n",
              "      <td>-0.193563</td>\n",
              "      <td>-0.136547</td>\n",
              "      <td>-0.044610</td>\n",
              "      <td>-1.471851</td>\n",
              "      <td>0.299420</td>\n",
              "      <td>-0.447038</td>\n",
              "      <td>0.065017</td>\n",
              "      <td>-0.903609</td>\n",
              "      <td>-0.798323</td>\n",
              "      <td>0.777325</td>\n",
              "      <td>0.567004</td>\n",
              "      <td>-1.292424</td>\n",
              "      <td>-1.835557</td>\n",
              "      <td>0.267809</td>\n",
              "      <td>-0.676375</td>\n",
              "      <td>1.297130</td>\n",
              "      <td>-0.277600</td>\n",
              "      <td>-0.458910</td>\n",
              "      <td>0.618104</td>\n",
              "      <td>-0.431244</td>\n",
              "      <td>1.781078</td>\n",
              "      <td>1.908755</td>\n",
              "      <td>-0.224010</td>\n",
              "      <td>1.282179</td>\n",
              "      <td>-1.551742</td>\n",
              "      <td>-0.388800</td>\n",
              "      <td>0.018078</td>\n",
              "      <td>0.019746</td>\n",
              "      <td>-0.189908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175643</th>\n",
              "      <td>0.581475</td>\n",
              "      <td>1.145707</td>\n",
              "      <td>-0.464983</td>\n",
              "      <td>-1.365275</td>\n",
              "      <td>-0.886248</td>\n",
              "      <td>0.305657</td>\n",
              "      <td>0.066829</td>\n",
              "      <td>-0.273461</td>\n",
              "      <td>-0.153975</td>\n",
              "      <td>-0.720206</td>\n",
              "      <td>0.881148</td>\n",
              "      <td>0.098351</td>\n",
              "      <td>0.322958</td>\n",
              "      <td>1.307987</td>\n",
              "      <td>-0.171925</td>\n",
              "      <td>-1.092521</td>\n",
              "      <td>1.037590</td>\n",
              "      <td>-0.323926</td>\n",
              "      <td>-0.816001</td>\n",
              "      <td>1.463272</td>\n",
              "      <td>0.145879</td>\n",
              "      <td>0.755524</td>\n",
              "      <td>2.356802</td>\n",
              "      <td>-0.473290</td>\n",
              "      <td>-0.455927</td>\n",
              "      <td>1.229323</td>\n",
              "      <td>0.632015</td>\n",
              "      <td>-0.113455</td>\n",
              "      <td>-0.257673</td>\n",
              "      <td>-0.253277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32164</th>\n",
              "      <td>-1.224452</td>\n",
              "      <td>-0.132705</td>\n",
              "      <td>0.713528</td>\n",
              "      <td>0.633126</td>\n",
              "      <td>0.630864</td>\n",
              "      <td>0.100135</td>\n",
              "      <td>0.231616</td>\n",
              "      <td>0.130995</td>\n",
              "      <td>-1.418178</td>\n",
              "      <td>-0.417453</td>\n",
              "      <td>0.428285</td>\n",
              "      <td>1.664365</td>\n",
              "      <td>1.082977</td>\n",
              "      <td>0.337247</td>\n",
              "      <td>0.279745</td>\n",
              "      <td>0.476292</td>\n",
              "      <td>-0.411223</td>\n",
              "      <td>-0.270911</td>\n",
              "      <td>-0.000925</td>\n",
              "      <td>0.079491</td>\n",
              "      <td>-0.446114</td>\n",
              "      <td>2.304290</td>\n",
              "      <td>0.332887</td>\n",
              "      <td>0.582661</td>\n",
              "      <td>0.332978</td>\n",
              "      <td>-2.285405</td>\n",
              "      <td>-1.196946</td>\n",
              "      <td>0.395602</td>\n",
              "      <td>-0.122872</td>\n",
              "      <td>-0.273268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79783</th>\n",
              "      <td>-0.771538</td>\n",
              "      <td>0.677347</td>\n",
              "      <td>-0.593270</td>\n",
              "      <td>0.134047</td>\n",
              "      <td>-0.819728</td>\n",
              "      <td>-0.374058</td>\n",
              "      <td>0.793413</td>\n",
              "      <td>-0.989350</td>\n",
              "      <td>0.295608</td>\n",
              "      <td>-0.421328</td>\n",
              "      <td>0.672262</td>\n",
              "      <td>-0.039082</td>\n",
              "      <td>-0.728508</td>\n",
              "      <td>-0.042104</td>\n",
              "      <td>-0.215013</td>\n",
              "      <td>0.938789</td>\n",
              "      <td>2.189987</td>\n",
              "      <td>-0.602120</td>\n",
              "      <td>-0.197767</td>\n",
              "      <td>0.977359</td>\n",
              "      <td>0.230517</td>\n",
              "      <td>0.499247</td>\n",
              "      <td>1.235857</td>\n",
              "      <td>-0.528168</td>\n",
              "      <td>-2.753810</td>\n",
              "      <td>0.998054</td>\n",
              "      <td>0.138978</td>\n",
              "      <td>0.089492</td>\n",
              "      <td>0.003362</td>\n",
              "      <td>-0.133535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107026</th>\n",
              "      <td>-0.517390</td>\n",
              "      <td>-0.161335</td>\n",
              "      <td>0.702321</td>\n",
              "      <td>0.761409</td>\n",
              "      <td>-0.019016</td>\n",
              "      <td>0.180568</td>\n",
              "      <td>-0.574303</td>\n",
              "      <td>0.599706</td>\n",
              "      <td>-0.067450</td>\n",
              "      <td>-0.410422</td>\n",
              "      <td>-0.483954</td>\n",
              "      <td>-0.433314</td>\n",
              "      <td>0.084132</td>\n",
              "      <td>0.714155</td>\n",
              "      <td>-0.580312</td>\n",
              "      <td>0.941704</td>\n",
              "      <td>0.527760</td>\n",
              "      <td>-0.179441</td>\n",
              "      <td>-0.103065</td>\n",
              "      <td>0.049886</td>\n",
              "      <td>0.207703</td>\n",
              "      <td>-0.382678</td>\n",
              "      <td>-0.981056</td>\n",
              "      <td>-0.103749</td>\n",
              "      <td>-0.019104</td>\n",
              "      <td>-0.181466</td>\n",
              "      <td>0.191556</td>\n",
              "      <td>0.615710</td>\n",
              "      <td>0.290262</td>\n",
              "      <td>-0.338876</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Time        V1        V2  ...       V27       V28    Amount\n",
              "212834  0.930700 -0.193563 -0.136547  ...  0.018078  0.019746 -0.189908\n",
              "175643  0.581475  1.145707 -0.464983  ... -0.113455 -0.257673 -0.253277\n",
              "32164  -1.224452 -0.132705  0.713528  ...  0.395602 -0.122872 -0.273268\n",
              "79783  -0.771538  0.677347 -0.593270  ...  0.089492  0.003362 -0.133535\n",
              "107026 -0.517390 -0.161335  0.702321  ...  0.615710  0.290262 -0.338876\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLV3ClFnrzIF",
        "outputId": "bc43173f-311b-4c5c-a7cd-8613d1743bf3"
      },
      "source": [
        "anomalyScoresPCA = anomalyScores(X_train, X_train_PCA_inverse)\n",
        "anomalyScoresPCA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "212834    0.000048\n",
              "175643    0.000219\n",
              "32164     0.000456\n",
              "79783     0.000046\n",
              "107026    0.000015\n",
              "            ...   \n",
              "48687     0.000113\n",
              "159608    0.000146\n",
              "205176    0.000054\n",
              "197673    0.000780\n",
              "151952    0.000346\n",
              "Length: 227845, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "cx608Hywt-eJ",
        "outputId": "3adc56ba-8a8c-484d-f1de-b4f3526f6ab4"
      },
      "source": [
        "import h2o\n",
        "h2o.init()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
            "Attempting to start a local H2O server...\n",
            "  Java Version: openjdk version \"11.0.10\" 2021-01-19; OpenJDK Runtime Environment (build 11.0.10+9-Ubuntu-0ubuntu1.18.04); OpenJDK 64-Bit Server VM (build 11.0.10+9-Ubuntu-0ubuntu1.18.04, mixed mode, sharing)\n",
            "  Starting server from /usr/local/lib/python3.7/dist-packages/h2o/backend/bin/h2o.jar\n",
            "  Ice root: /tmp/tmph_1ei845\n",
            "  JVM stdout: /tmp/tmph_1ei845/h2o_unknownUser_started_from_python.out\n",
            "  JVM stderr: /tmp/tmph_1ei845/h2o_unknownUser_started_from_python.err\n",
            "  Server is running at http://127.0.0.1:54321\n",
            "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>02 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.32.0.4</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>1 month and 3 days </td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_cmabb3</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>3.180 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>accepting new members, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://127.0.0.1:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>H2O_API_Extensions:</td>\n",
              "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.7.10 final</td></tr></table></div>"
            ],
            "text/plain": [
              "--------------------------  ------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         02 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.32.0.4\n",
              "H2O_cluster_version_age:    1 month and 3 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_cmabb3\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    3.180 Gb\n",
              "H2O_cluster_total_cores:    2\n",
              "H2O_cluster_allowed_cores:  2\n",
              "H2O_cluster_status:         accepting new members, healthy\n",
              "H2O_connection_url:         http://127.0.0.1:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
              "H2O_internal_security:      False\n",
              "H2O_API_Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
              "Python_version:             3.7.10 final\n",
              "--------------------------  ------------------------------------------------------------------"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "JiV0qQbatNKL",
        "outputId": "a7036e53-d61f-4d57-c267-8c75ce16e729"
      },
      "source": [
        "anomalyScoresPCA_h2o = h2o.H2OFrame(pd.DataFrame(anomalyScoresPCA))\n",
        "anomalyScoresPCA_h2o"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th style=\"text-align: right;\">          0</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td style=\"text-align: right;\">4.79664e-05</td></tr>\n",
              "<tr><td style=\"text-align: right;\">0.000218706</td></tr>\n",
              "<tr><td style=\"text-align: right;\">0.000455847</td></tr>\n",
              "<tr><td style=\"text-align: right;\">4.57365e-05</td></tr>\n",
              "<tr><td style=\"text-align: right;\">1.53108e-05</td></tr>\n",
              "<tr><td style=\"text-align: right;\">8.46817e-06</td></tr>\n",
              "<tr><td style=\"text-align: right;\">4.67493e-05</td></tr>\n",
              "<tr><td style=\"text-align: right;\">4.91935e-05</td></tr>\n",
              "<tr><td style=\"text-align: right;\">0.000140031</td></tr>\n",
              "<tr><td style=\"text-align: right;\">0.000139547</td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "7NZnPegpwECs",
        "outputId": "0716b6d7-799f-4209-89aa-3ae04e08ec81"
      },
      "source": [
        "anomalyScoresPCA_h2o.hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "3HqzGiP9uTjK",
        "outputId": "f496e89d-6265-40bc-e425-e12a5986da9a"
      },
      "source": [
        "anomalyScoresPCA_h2o.quantile()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th style=\"text-align: right;\">  Probs</th><th style=\"text-align: right;\">  0Quantiles</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td style=\"text-align: right;\">  0.01 </td><td style=\"text-align: right;\"> 6.44003e-07</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.1  </td><td style=\"text-align: right;\"> 7.35062e-06</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.25 </td><td style=\"text-align: right;\"> 2.05486e-05</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.333</td><td style=\"text-align: right;\"> 3.15163e-05</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.5  </td><td style=\"text-align: right;\"> 6.3834e-05 </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.667</td><td style=\"text-align: right;\"> 0.000124889</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.75 </td><td style=\"text-align: right;\"> 0.000180762</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.9  </td><td style=\"text-align: right;\"> 0.000463321</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.99 </td><td style=\"text-align: right;\"> 0.00164339 </td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8ItYYVtvrZI",
        "outputId": "12004b45-cb96-420b-e994-f49ef6ff74c7"
      },
      "source": [
        "y_train.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.998271\n",
              "1    0.001729\n",
              "Name: Class, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "id": "VstwCS1lvcCE",
        "outputId": "e8f10109-c985-4643-b15e-b9cfb21dba62"
      },
      "source": [
        "anomalyScoresPCA_h2o.quantile(prob=[0.998271]) # ti le fraud"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th style=\"text-align: right;\">   Probs</th><th style=\"text-align: right;\">  0Quantiles</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td style=\"text-align: right;\">0.998271</td><td style=\"text-align: right;\">   0.0213515</td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq7Ao_eOxjNf"
      },
      "source": [
        "Predict in train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPwEPi7DuTln",
        "outputId": "a29c843d-c3a6-42fa-954c-1521a74027e3"
      },
      "source": [
        "#Best Threshold=0.027314, F-Score=0.799\n",
        "best_threshold=0.0213515 # Normal threshold if not tunning\n",
        "y_pred_new = [1 if x >= best_threshold else 0 for x in anomalyScoresPCA]\n",
        "confmat = confusion_matrix(y_train,y_pred_new)\n",
        "print('accuracy_score: \\n',accuracy_score(y_train, y_pred_new))\n",
        "print('confusion_matrix \\n',confmat)\n",
        "print('classification_report \\n',classification_report(y_train, y_pred_new))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_score: \n",
            " 0.9992538787333495\n",
            "confusion_matrix \n",
            " [[227366     85]\n",
            " [    85    309]]\n",
            "classification_report \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    227451\n",
            "           1       0.78      0.78      0.78       394\n",
            "\n",
            "    accuracy                           1.00    227845\n",
            "   macro avg       0.89      0.89      0.89    227845\n",
            "weighted avg       1.00      1.00      1.00    227845\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qd2x0DRrzL_",
        "outputId": "e8107f96-3b24-418d-bf38-3f75b6b3b71c"
      },
      "source": [
        "best_threshold=0.5 # Normal threshold if not tunning\n",
        "y_pred_new = [1 if x >= best_threshold else 0 for x in anomalyScoresPCA]\n",
        "confmat = confusion_matrix(y_train,y_pred_new)\n",
        "print('accuracy_score: \\n',accuracy_score(y_train, y_pred_new))\n",
        "print('confusion_matrix \\n',confmat)\n",
        "print('classification_report \\n',classification_report(y_train, y_pred_new))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_score: \n",
            " 0.9985209243125809\n",
            "confusion_matrix \n",
            " [[227435     16]\n",
            " [   321     73]]\n",
            "classification_report \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    227451\n",
            "           1       0.82      0.19      0.30       394\n",
            "\n",
            "    accuracy                           1.00    227845\n",
            "   macro avg       0.91      0.59      0.65    227845\n",
            "weighted avg       1.00      1.00      1.00    227845\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoGsnqeGxmje"
      },
      "source": [
        "Predict in test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uCxEM0cxokv",
        "outputId": "0b6e91f7-9645-4a2b-c4b8-9181db114f3d"
      },
      "source": [
        "X_test_PCA = pca.transform(X_test)\n",
        "X_test_PCA = pd.DataFrame(data=X_test_PCA, index=X_test.index)\n",
        "\n",
        "X_test_PCA_inverse = pca.inverse_transform(X_test_PCA)\n",
        "X_test_PCA_inverse = pd.DataFrame(data=X_test_PCA_inverse,index=X_test.index)\n",
        "anomalyScoresPCA = anomalyScores(X_test, X_test_PCA_inverse)\n",
        "\n",
        "best_threshold=0.0213515 # Normal threshold if not tunning\n",
        "y_pred_new = [1 if x >= best_threshold else 0 for x in anomalyScoresPCA]\n",
        "confmat = confusion_matrix(y_test,y_pred_new)\n",
        "print('accuracy_score: \\n',accuracy_score(y_test, y_pred_new))\n",
        "print('confusion_matrix \\n',confmat)\n",
        "print('classification_report \\n',classification_report(y_test, y_pred_new))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_score: \n",
            " 0.9992626663389628\n",
            "confusion_matrix \n",
            " [[56842    22]\n",
            " [   20    78]]\n",
            "classification_report \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.78      0.80      0.79        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.89      0.90      0.89     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHF0RvNLzwa4"
      },
      "source": [
        "## SparsePCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgv7iPQRrzNj"
      },
      "source": [
        "# SparsePCA\n",
        "from sklearn.decomposition import SparsePCA\n",
        "\n",
        "n_components = 27\n",
        "alpha = 0.0001\n",
        "random_state = 2018\n",
        "n_jobs = -1\n",
        "\n",
        "sparsePCA = SparsePCA(n_components=n_components, \\\n",
        "                alpha=alpha, random_state=random_state, n_jobs=n_jobs)\n",
        "\n",
        "sparsePCA.fit(X_train.loc[:,:])\n",
        "X_train_sparsePCA = sparsePCA.transform(X_train)\n",
        "X_train_sparsePCA = pd.DataFrame(data=X_train_sparsePCA, index=X_train.index)\n",
        "X_train_sparsePCA_inverse = np.array(X_train_sparsePCA). \\\n",
        "    dot(sparsePCA.components_) + np.array(X_train.mean(axis=0))\n",
        "X_train_sparsePCA_inverse = pd.DataFrame(data=X_train_sparsePCA_inverse,index=X_train.index)\n",
        "\n",
        "anomalyScoresPCA = anomalyScores(X_train, X_train_sparsePCA_inverse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "7UmvHFSprfWZ",
        "outputId": "526b2b40-9652-4002-a5b3-c59df0ea1e20"
      },
      "source": [
        "anomalyScoresPCA_h2o = h2o.H2OFrame(pd.DataFrame(anomalyScoresPCA))\n",
        "anomalyScoresPCA_h2o.quantile(prob=[0.998271]) # ti le fraud"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th style=\"text-align: right;\">   Probs</th><th style=\"text-align: right;\">  0Quantiles</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td style=\"text-align: right;\">0.998271</td><td style=\"text-align: right;\">   0.0214191</td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2MQP89my5rG",
        "outputId": "4505f510-3d80-4d0b-9b3e-9d1be0746dee"
      },
      "source": [
        "best_threshold=0.0214191\n",
        "y_pred_new = [1 if x >= best_threshold else 0 for x in anomalyScoresPCA]\n",
        "confmat = confusion_matrix(y_train,y_pred_new)\n",
        "print('accuracy_score: \\n',accuracy_score(y_train, y_pred_new))\n",
        "print('confusion_matrix \\n',confmat)\n",
        "print('classification_report \\n',classification_report(y_train, y_pred_new))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_score: \n",
            " 0.9992451008360947\n",
            "confusion_matrix \n",
            " [[227365     86]\n",
            " [    86    308]]\n",
            "classification_report \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    227451\n",
            "           1       0.78      0.78      0.78       394\n",
            "\n",
            "    accuracy                           1.00    227845\n",
            "   macro avg       0.89      0.89      0.89    227845\n",
            "weighted avg       1.00      1.00      1.00    227845\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY1DjWGgy9VK",
        "outputId": "db8ddf53-7324-4c9d-acb0-124bb2da520d"
      },
      "source": [
        "X_test_PCA = sparsePCA.transform(X_test)\n",
        "X_test_PCA = pd.DataFrame(data=X_test_PCA, index=X_test.index)\n",
        "\n",
        "X_test_PCA_inverse =  np.array(X_test_PCA). \\\n",
        "    dot(sparsePCA.components_) + np.array(X_test.mean(axis=0))\n",
        "X_test_PCA_inverse = pd.DataFrame(data=X_test_PCA_inverse,index=X_test.index)\n",
        "anomalyScoresPCA = anomalyScores(X_test, X_test_PCA_inverse)\n",
        "\n",
        "best_threshold=0.0214191\n",
        "y_pred_new = [1 if x >= best_threshold else 0 for x in anomalyScoresPCA]\n",
        "confmat = confusion_matrix(y_test,y_pred_new)\n",
        "print('accuracy_score: \\n',accuracy_score(y_test, y_pred_new))\n",
        "print('confusion_matrix \\n',confmat)\n",
        "print('classification_report \\n',classification_report(y_test, y_pred_new))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_score: \n",
            " 0.9992626663389628\n",
            "confusion_matrix \n",
            " [[56842    22]\n",
            " [   20    78]]\n",
            "classification_report \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.78      0.80      0.79        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.89      0.90      0.89     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XVcJmovg8Hx"
      },
      "source": [
        "# Auto encoder DL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBtZjsYGg_Ir"
      },
      "source": [
        "\n",
        "'''Main'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, time, re\n",
        "import pickle, gzip\n",
        "\n",
        "'''Data Viz'''\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "import matplotlib as mpl\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "'''Data Prep and Model Evaluation'''\n",
        "from sklearn import preprocessing as pp\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "'''Algos'''\n",
        "#import lightgbm as lgb\n",
        "\n",
        "'''TensorFlow and Keras'''\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "K = keras.backend\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization, Input, Lambda\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.losses import mse, binary_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX7aJer5g_LO",
        "outputId": "97ff0a72-b4d8-409c-90c9-3ded97fa40d8"
      },
      "source": [
        "import sys, sklearn\n",
        "print(f'sklearn    {sklearn.__version__}')\n",
        "print(f'tensorflow {tf.__version__}')\n",
        "print(f'keras      {keras.__version__}')\n",
        "print(f'numpy      {np.__version__}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sklearn    0.22.2.post1\n",
            "tensorflow 2.4.1\n",
            "keras      2.4.0\n",
            "numpy      1.19.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RdMYqVgg_Nx"
      },
      "source": [
        "# To make the output stable across runs\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7c356IWhe_P"
      },
      "source": [
        "file='https://media.githubusercontent.com/media/aapatel09/handson-unsupervised-learning/master/datasets/credit_card_data/credit_card.csv'\n",
        "data = pd.read_csv(file)\n",
        "\n",
        "dataX = data.copy().drop(['Class','Time'],axis=1)\n",
        "dataY = data['Class'].copy()\n",
        "\n",
        "featuresToScale = dataX.columns\n",
        "sX =pp.StandardScaler(copy=True)\n",
        "dataX.loc[:,featuresToScale] = sX.fit_transform(dataX[featuresToScale])\n",
        "\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(dataX, dataY, test_size=0.20, \\\n",
        "                    random_state=2018, stratify=dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-4MPw-DhfBl"
      },
      "source": [
        "def anomalyScores(originalDF, reducedDF):\n",
        "    loss = np.sum((np.array(originalDF) - \n",
        "                   np.array(reducedDF))**2, axis=1)\n",
        "    loss = pd.Series(data=loss,index=originalDF.index)\n",
        "    loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HZfadHijLib"
      },
      "source": [
        "Model Three\n",
        "\n",
        "Three layer undercomplete autoencoder with linear activation.\n",
        "With 28 and 27 nodes in the two hidden layers, respectively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArU6v25vhfEM",
        "outputId": "c50a1359-e430-46d6-e609-d8719b13cda8"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "test_scores = []\n",
        "\n",
        "# Call neural network API\n",
        "model = Sequential()\n",
        "\n",
        "# Apply linear activation function to input layer\n",
        "# Generate first hidden layer with 27 nodes\n",
        "# Generate second hidden layer with 28 nodes\n",
        "model.add(Dense(units=28, activation='linear',input_dim=29))\n",
        "model.add(Dense(units=27, activation='linear'))\n",
        "\n",
        "# Apply linear activation function to second hidden layer\n",
        "# Generate output layer with 29 nodes\n",
        "model.add(Dense(units=29, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(x=X_train, y=y_train,\n",
        "                    epochs=num_epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True,\n",
        "                    validation_data=(X_train, y_train),\n",
        "                    verbose=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0423 - accuracy: 0.0502 - val_loss: 0.0018 - val_accuracy: 0.0037\n",
            "Epoch 2/10\n",
            "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0011 - accuracy: 0.0227 - val_loss: 8.8404e-04 - val_accuracy: 0.0096\n",
            "Epoch 3/10\n",
            "7121/7121 [==============================] - 15s 2ms/step - loss: 9.1330e-04 - accuracy: 0.0219 - val_loss: 0.0012 - val_accuracy: 0.0175\n",
            "Epoch 4/10\n",
            "7121/7121 [==============================] - 15s 2ms/step - loss: 8.9984e-04 - accuracy: 0.0178 - val_loss: 8.6748e-04 - val_accuracy: 0.0704\n",
            "Epoch 5/10\n",
            "7121/7121 [==============================] - 14s 2ms/step - loss: 9.7712e-04 - accuracy: 0.0307 - val_loss: 8.4466e-04 - val_accuracy: 0.0047\n",
            "Epoch 6/10\n",
            "7121/7121 [==============================] - 14s 2ms/step - loss: 9.7450e-04 - accuracy: 0.0351 - val_loss: 8.6196e-04 - val_accuracy: 0.0013\n",
            "Epoch 7/10\n",
            "7121/7121 [==============================] - 15s 2ms/step - loss: 9.7899e-04 - accuracy: 0.0351 - val_loss: 8.8140e-04 - val_accuracy: 0.0058\n",
            "Epoch 8/10\n",
            "7121/7121 [==============================] - 16s 2ms/step - loss: 9.5312e-04 - accuracy: 0.0176 - val_loss: 8.9339e-04 - val_accuracy: 0.0509\n",
            "Epoch 9/10\n",
            "7121/7121 [==============================] - 14s 2ms/step - loss: 9.4745e-04 - accuracy: 0.0197 - val_loss: 9.4874e-04 - val_accuracy: 0.0186\n",
            "Epoch 10/10\n",
            "7121/7121 [==============================] - 15s 2ms/step - loss: 8.2025e-04 - accuracy: 0.0130 - val_loss: 8.3934e-04 - val_accuracy: 0.0329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA1vrckql1hX",
        "outputId": "be7ae1cb-64a7-49b9-efed-dd0b53bbb9f6"
      },
      "source": [
        "!pip install h2o"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting h2o\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/d4/5c07504a392e94786e7cf33554d961ac4b2863aa22a07b8579940ea1f6b5/h2o-3.32.0.4.tar.gz (164.6MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164.6MB 48kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from h2o) (2.23.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from h2o) (0.8.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from h2o) (0.16.0)\n",
            "Collecting colorama>=0.3.8\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (1.24.3)\n",
            "Building wheels for collected packages: h2o\n",
            "  Building wheel for h2o (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for h2o: filename=h2o-3.32.0.4-py2.py3-none-any.whl size=164670979 sha256=fb9b99795d860c4a0432b17e2d9c95c1ef42eb4e97c0d464a223b4a99370f923\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/f4/0d/c9bb958d70c2e015c968cb91cbd7f1b486933056d422337d75\n",
            "Successfully built h2o\n",
            "Installing collected packages: colorama, h2o\n",
            "Successfully installed colorama-0.4.4 h2o-3.32.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "FnAtlCg0lrXU",
        "outputId": "74cd0244-55e1-4d92-d8cb-772dbdc3a48e"
      },
      "source": [
        "import h2o\n",
        "h2o.init()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
            "Attempting to start a local H2O server...\n",
            "  Java Version: openjdk version \"11.0.10\" 2021-01-19; OpenJDK Runtime Environment (build 11.0.10+9-Ubuntu-0ubuntu1.18.04); OpenJDK 64-Bit Server VM (build 11.0.10+9-Ubuntu-0ubuntu1.18.04, mixed mode, sharing)\n",
            "  Starting server from /usr/local/lib/python3.7/dist-packages/h2o/backend/bin/h2o.jar\n",
            "  Ice root: /tmp/tmpd5ptwz9s\n",
            "  JVM stdout: /tmp/tmpd5ptwz9s/h2o_unknownUser_started_from_python.out\n",
            "  JVM stderr: /tmp/tmpd5ptwz9s/h2o_unknownUser_started_from_python.err\n",
            "  Server is running at http://127.0.0.1:54321\n",
            "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>03 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.32.0.4</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>1 month and 3 days </td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_2kxmgc</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>3.180 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>accepting new members, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://127.0.0.1:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>H2O_API_Extensions:</td>\n",
              "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.7.10 final</td></tr></table></div>"
            ],
            "text/plain": [
              "--------------------------  ------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         03 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.32.0.4\n",
              "H2O_cluster_version_age:    1 month and 3 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_2kxmgc\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    3.180 Gb\n",
              "H2O_cluster_total_cores:    2\n",
              "H2O_cluster_allowed_cores:  2\n",
              "H2O_cluster_status:         accepting new members, healthy\n",
              "H2O_connection_url:         http://127.0.0.1:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
              "H2O_internal_security:      False\n",
              "H2O_API_Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
              "Python_version:             3.7.10 final\n",
              "--------------------------  ------------------------------------------------------------------"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "7dnxrxOPmWVS",
        "outputId": "60d38c2b-3bf4-48e9-911f-f917423828dd"
      },
      "source": [
        "# Evaluate on train set\n",
        "predictions = model.predict(X_train, verbose=1) # prediction is nearly the same as X_train\n",
        "predictions = pd.DataFrame(data=predictions,index=X_train.index)\n",
        "predictions.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7121/7121 [==============================] - 7s 1ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>212834</th>\n",
              "      <td>0.000892</td>\n",
              "      <td>-0.000771</td>\n",
              "      <td>-0.000132</td>\n",
              "      <td>-0.000731</td>\n",
              "      <td>0.000409</td>\n",
              "      <td>0.000350</td>\n",
              "      <td>0.001214</td>\n",
              "      <td>-0.001293</td>\n",
              "      <td>-0.000168</td>\n",
              "      <td>-0.000470</td>\n",
              "      <td>-0.000780</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.001509</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.001013</td>\n",
              "      <td>0.001370</td>\n",
              "      <td>-0.000879</td>\n",
              "      <td>0.000517</td>\n",
              "      <td>0.002528</td>\n",
              "      <td>0.000232</td>\n",
              "      <td>0.001192</td>\n",
              "      <td>0.000574</td>\n",
              "      <td>0.000637</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.000861</td>\n",
              "      <td>0.002962</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>-0.000003</td>\n",
              "      <td>-0.001984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175643</th>\n",
              "      <td>0.000345</td>\n",
              "      <td>-0.000047</td>\n",
              "      <td>0.000655</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>-0.000186</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>-0.000158</td>\n",
              "      <td>0.000738</td>\n",
              "      <td>-0.000446</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000398</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>-0.000196</td>\n",
              "      <td>-0.000296</td>\n",
              "      <td>-0.000141</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>-0.000308</td>\n",
              "      <td>0.000815</td>\n",
              "      <td>0.000579</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>-0.000433</td>\n",
              "      <td>-0.000347</td>\n",
              "      <td>-0.000413</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>-0.000459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32164</th>\n",
              "      <td>0.003402</td>\n",
              "      <td>0.002823</td>\n",
              "      <td>0.002259</td>\n",
              "      <td>0.002218</td>\n",
              "      <td>0.002788</td>\n",
              "      <td>0.002384</td>\n",
              "      <td>0.002400</td>\n",
              "      <td>0.001947</td>\n",
              "      <td>0.002298</td>\n",
              "      <td>0.002736</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>0.002197</td>\n",
              "      <td>0.003172</td>\n",
              "      <td>0.002505</td>\n",
              "      <td>0.002462</td>\n",
              "      <td>0.002865</td>\n",
              "      <td>0.002564</td>\n",
              "      <td>0.003177</td>\n",
              "      <td>0.003627</td>\n",
              "      <td>0.002733</td>\n",
              "      <td>0.003624</td>\n",
              "      <td>0.002510</td>\n",
              "      <td>0.003502</td>\n",
              "      <td>0.003308</td>\n",
              "      <td>0.003183</td>\n",
              "      <td>0.004588</td>\n",
              "      <td>0.003929</td>\n",
              "      <td>0.002555</td>\n",
              "      <td>0.001302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79783</th>\n",
              "      <td>0.000690</td>\n",
              "      <td>-0.000109</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>0.000521</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>-0.000176</td>\n",
              "      <td>-0.000185</td>\n",
              "      <td>-0.000469</td>\n",
              "      <td>-0.000224</td>\n",
              "      <td>-0.000592</td>\n",
              "      <td>0.001265</td>\n",
              "      <td>0.000873</td>\n",
              "      <td>0.000553</td>\n",
              "      <td>0.000979</td>\n",
              "      <td>-0.000490</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>0.001635</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.001166</td>\n",
              "      <td>0.000915</td>\n",
              "      <td>0.000512</td>\n",
              "      <td>0.000899</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.001772</td>\n",
              "      <td>0.000815</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>-0.001483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107026</th>\n",
              "      <td>0.002067</td>\n",
              "      <td>0.000457</td>\n",
              "      <td>0.001548</td>\n",
              "      <td>0.001501</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.002437</td>\n",
              "      <td>0.002059</td>\n",
              "      <td>0.001437</td>\n",
              "      <td>0.001543</td>\n",
              "      <td>0.001447</td>\n",
              "      <td>0.001730</td>\n",
              "      <td>0.002184</td>\n",
              "      <td>0.002102</td>\n",
              "      <td>0.001810</td>\n",
              "      <td>0.002378</td>\n",
              "      <td>0.002072</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.002552</td>\n",
              "      <td>0.002423</td>\n",
              "      <td>0.001581</td>\n",
              "      <td>0.001981</td>\n",
              "      <td>0.002110</td>\n",
              "      <td>0.001965</td>\n",
              "      <td>0.001405</td>\n",
              "      <td>0.002318</td>\n",
              "      <td>0.002944</td>\n",
              "      <td>0.001714</td>\n",
              "      <td>0.001335</td>\n",
              "      <td>0.001117</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         1         2   ...        26        27        28\n",
              "212834  0.000892 -0.000771 -0.000132  ...  0.001041 -0.000003 -0.001984\n",
              "175643  0.000345 -0.000047  0.000655  ... -0.000413  0.000027 -0.000459\n",
              "32164   0.003402  0.002823  0.002259  ...  0.003929  0.002555  0.001302\n",
              "79783   0.000690 -0.000109  0.000094  ...  0.000815  0.000228 -0.001483\n",
              "107026  0.002067  0.000457  0.001548  ...  0.001714  0.001335  0.001117\n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "AwoiRbmesonv",
        "outputId": "3e515c20-57f5-4823-c240-2c7fc447f86e"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>212834</th>\n",
              "      <td>-0.193563</td>\n",
              "      <td>-0.136547</td>\n",
              "      <td>-0.044610</td>\n",
              "      <td>-1.471851</td>\n",
              "      <td>0.299420</td>\n",
              "      <td>-0.447038</td>\n",
              "      <td>0.065017</td>\n",
              "      <td>-0.903609</td>\n",
              "      <td>-0.798323</td>\n",
              "      <td>0.777325</td>\n",
              "      <td>0.567004</td>\n",
              "      <td>-1.292424</td>\n",
              "      <td>-1.835557</td>\n",
              "      <td>0.267809</td>\n",
              "      <td>-0.676375</td>\n",
              "      <td>1.297130</td>\n",
              "      <td>-0.277600</td>\n",
              "      <td>-0.458910</td>\n",
              "      <td>0.618104</td>\n",
              "      <td>-0.431244</td>\n",
              "      <td>1.781078</td>\n",
              "      <td>1.908755</td>\n",
              "      <td>-0.224010</td>\n",
              "      <td>1.282179</td>\n",
              "      <td>-1.551742</td>\n",
              "      <td>-0.388800</td>\n",
              "      <td>0.018078</td>\n",
              "      <td>0.019746</td>\n",
              "      <td>-0.189908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175643</th>\n",
              "      <td>1.145707</td>\n",
              "      <td>-0.464983</td>\n",
              "      <td>-1.365275</td>\n",
              "      <td>-0.886248</td>\n",
              "      <td>0.305657</td>\n",
              "      <td>0.066829</td>\n",
              "      <td>-0.273461</td>\n",
              "      <td>-0.153975</td>\n",
              "      <td>-0.720206</td>\n",
              "      <td>0.881148</td>\n",
              "      <td>0.098351</td>\n",
              "      <td>0.322958</td>\n",
              "      <td>1.307987</td>\n",
              "      <td>-0.171925</td>\n",
              "      <td>-1.092521</td>\n",
              "      <td>1.037590</td>\n",
              "      <td>-0.323926</td>\n",
              "      <td>-0.816001</td>\n",
              "      <td>1.463272</td>\n",
              "      <td>0.145879</td>\n",
              "      <td>0.755524</td>\n",
              "      <td>2.356802</td>\n",
              "      <td>-0.473290</td>\n",
              "      <td>-0.455927</td>\n",
              "      <td>1.229323</td>\n",
              "      <td>0.632015</td>\n",
              "      <td>-0.113455</td>\n",
              "      <td>-0.257673</td>\n",
              "      <td>-0.253277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32164</th>\n",
              "      <td>-0.132705</td>\n",
              "      <td>0.713528</td>\n",
              "      <td>0.633126</td>\n",
              "      <td>0.630864</td>\n",
              "      <td>0.100135</td>\n",
              "      <td>0.231616</td>\n",
              "      <td>0.130995</td>\n",
              "      <td>-1.418178</td>\n",
              "      <td>-0.417453</td>\n",
              "      <td>0.428285</td>\n",
              "      <td>1.664365</td>\n",
              "      <td>1.082977</td>\n",
              "      <td>0.337247</td>\n",
              "      <td>0.279745</td>\n",
              "      <td>0.476292</td>\n",
              "      <td>-0.411223</td>\n",
              "      <td>-0.270911</td>\n",
              "      <td>-0.000925</td>\n",
              "      <td>0.079491</td>\n",
              "      <td>-0.446114</td>\n",
              "      <td>2.304290</td>\n",
              "      <td>0.332887</td>\n",
              "      <td>0.582661</td>\n",
              "      <td>0.332978</td>\n",
              "      <td>-2.285405</td>\n",
              "      <td>-1.196946</td>\n",
              "      <td>0.395602</td>\n",
              "      <td>-0.122872</td>\n",
              "      <td>-0.273268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79783</th>\n",
              "      <td>0.677347</td>\n",
              "      <td>-0.593270</td>\n",
              "      <td>0.134047</td>\n",
              "      <td>-0.819728</td>\n",
              "      <td>-0.374058</td>\n",
              "      <td>0.793413</td>\n",
              "      <td>-0.989350</td>\n",
              "      <td>0.295608</td>\n",
              "      <td>-0.421328</td>\n",
              "      <td>0.672262</td>\n",
              "      <td>-0.039082</td>\n",
              "      <td>-0.728508</td>\n",
              "      <td>-0.042104</td>\n",
              "      <td>-0.215013</td>\n",
              "      <td>0.938789</td>\n",
              "      <td>2.189987</td>\n",
              "      <td>-0.602120</td>\n",
              "      <td>-0.197767</td>\n",
              "      <td>0.977359</td>\n",
              "      <td>0.230517</td>\n",
              "      <td>0.499247</td>\n",
              "      <td>1.235857</td>\n",
              "      <td>-0.528168</td>\n",
              "      <td>-2.753810</td>\n",
              "      <td>0.998054</td>\n",
              "      <td>0.138978</td>\n",
              "      <td>0.089492</td>\n",
              "      <td>0.003362</td>\n",
              "      <td>-0.133535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107026</th>\n",
              "      <td>-0.161335</td>\n",
              "      <td>0.702321</td>\n",
              "      <td>0.761409</td>\n",
              "      <td>-0.019016</td>\n",
              "      <td>0.180568</td>\n",
              "      <td>-0.574303</td>\n",
              "      <td>0.599706</td>\n",
              "      <td>-0.067450</td>\n",
              "      <td>-0.410422</td>\n",
              "      <td>-0.483954</td>\n",
              "      <td>-0.433314</td>\n",
              "      <td>0.084132</td>\n",
              "      <td>0.714155</td>\n",
              "      <td>-0.580312</td>\n",
              "      <td>0.941704</td>\n",
              "      <td>0.527760</td>\n",
              "      <td>-0.179441</td>\n",
              "      <td>-0.103065</td>\n",
              "      <td>0.049886</td>\n",
              "      <td>0.207703</td>\n",
              "      <td>-0.382678</td>\n",
              "      <td>-0.981056</td>\n",
              "      <td>-0.103749</td>\n",
              "      <td>-0.019104</td>\n",
              "      <td>-0.181466</td>\n",
              "      <td>0.191556</td>\n",
              "      <td>0.615710</td>\n",
              "      <td>0.290262</td>\n",
              "      <td>-0.338876</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              V1        V2        V3  ...       V27       V28    Amount\n",
              "212834 -0.193563 -0.136547 -0.044610  ...  0.018078  0.019746 -0.189908\n",
              "175643  1.145707 -0.464983 -1.365275  ... -0.113455 -0.257673 -0.253277\n",
              "32164  -0.132705  0.713528  0.633126  ...  0.395602 -0.122872 -0.273268\n",
              "79783   0.677347 -0.593270  0.134047  ...  0.089492  0.003362 -0.133535\n",
              "107026 -0.161335  0.702321  0.761409  ...  0.615710  0.290262 -0.338876\n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMsKYwfGtUnk",
        "outputId": "09a42730-6d0b-449e-c71c-ab90fe693339"
      },
      "source": [
        "originalDF = X_train\n",
        "reducedDF = predictions\n",
        "loss = np.sum((np.array(originalDF) - \n",
        "                np.array(reducedDF))**2, axis=1)\n",
        "loss = pd.Series(data=loss,index=originalDF.index)\n",
        "loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "212834    0.000419\n",
              "175643    0.000360\n",
              "32164     0.000360\n",
              "79783     0.000381\n",
              "107026    0.000044\n",
              "            ...   \n",
              "48687     0.000331\n",
              "159608    0.000166\n",
              "205176    0.001988\n",
              "197673    0.000749\n",
              "151952    0.000664\n",
              "Length: 227845, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "FfpfiSlshfGj",
        "outputId": "9305425f-31f8-46fd-c2cb-fa2e650f9c1a"
      },
      "source": [
        "#anomalyScores = anomalyScores(X_train, predictions)\n",
        "anomalyScores = loss\n",
        "anomalyScores_h2o = h2o.H2OFrame(pd.DataFrame(anomalyScores))\n",
        "anomalyScores_h2o.quantile() # ti le fraud\n",
        "anomalyScores_h2o.quantile(prob=[0.998271]) # ti le fraud"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th style=\"text-align: right;\">  Probs</th><th style=\"text-align: right;\">  0Quantiles</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td style=\"text-align: right;\">  0.01 </td><td style=\"text-align: right;\"> 4.2841e-05 </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.1  </td><td style=\"text-align: right;\"> 9.31087e-05</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.25 </td><td style=\"text-align: right;\"> 0.000156913</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.333</td><td style=\"text-align: right;\"> 0.000194149</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.5  </td><td style=\"text-align: right;\"> 0.000278055</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.667</td><td style=\"text-align: right;\"> 0.000390163</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.75 </td><td style=\"text-align: right;\"> 0.000461186</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.9  </td><td style=\"text-align: right;\"> 0.000741543</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  0.99 </td><td style=\"text-align: right;\"> 0.00476261 </td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "id": "0y1cH3_0szEs",
        "outputId": "256dc7ba-cc06-4307-c0b8-70d7ff587e48"
      },
      "source": [
        "anomalyScores_h2o.quantile(prob=[0.998271]) # ti le fraud"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th style=\"text-align: right;\">   Probs</th><th style=\"text-align: right;\">  0Quantiles</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td style=\"text-align: right;\">0.998271</td><td style=\"text-align: right;\">   0.0183468</td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7jP84uzhfJK",
        "outputId": "c87b4df6-3540-43ee-ac15-38ed2509a362"
      },
      "source": [
        "best_threshold=0.0183468\n",
        "y_pred_new = [1 if x >= best_threshold else 0 for x in anomalyScores]\n",
        "confmat = confusion_matrix(y_train,y_pred_new)\n",
        "print('accuracy_score: \\n',accuracy_score(y_train, y_pred_new))\n",
        "print('confusion_matrix \\n',confmat)\n",
        "print('classification_report \\n',classification_report(y_train, y_pred_new))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_score: \n",
            " 0.997340297131822\n",
            "confusion_matrix \n",
            " [[227148    303]\n",
            " [   303     91]]\n",
            "classification_report \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    227451\n",
            "           1       0.23      0.23      0.23       394\n",
            "\n",
            "    accuracy                           1.00    227845\n",
            "   macro avg       0.61      0.61      0.61    227845\n",
            "weighted avg       1.00      1.00      1.00    227845\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDOQVKU-vdr2",
        "outputId": "12c643c7-bf1b-44bb-ef39-cfc2754368e6"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "212834    0\n",
              "175643    0\n",
              "32164     0\n",
              "79783     0\n",
              "107026    0\n",
              "         ..\n",
              "48687     0\n",
              "159608    0\n",
              "205176    0\n",
              "197673    0\n",
              "151952    0\n",
              "Name: Class, Length: 227845, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsqcHIFuvXvt",
        "outputId": "50bc17e7-c5d7-4881-cb1e-c1f63fe17831"
      },
      "source": [
        "len(y_pred_new) # list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "227845"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot6-0IiShfLx",
        "outputId": "695dba9f-1374-4788-d78a-fc8ab34a70c8"
      },
      "source": [
        "# Evaluate on test set\n",
        "predictions = model.predict(X_test, verbose=1) # prediction is nearly the same as X_train\n",
        "predictions = pd.DataFrame(data=predictions,index=X_test.index)\n",
        "\n",
        "originalDF = X_test\n",
        "reducedDF = predictions\n",
        "loss = np.sum((np.array(originalDF) - \n",
        "                np.array(reducedDF))**2, axis=1)\n",
        "loss = pd.Series(data=loss,index=originalDF.index)\n",
        "loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
        "anomalyScores=loss\n",
        "best_threshold=0.0183468\n",
        "y_pred_new = [1 if x >= best_threshold else 0 for x in anomalyScores]\n",
        "confmat = confusion_matrix(y_test,y_pred_new)\n",
        "print('accuracy_score: \\n',accuracy_score(y_test, y_pred_new))\n",
        "print('confusion_matrix \\n',confmat)\n",
        "print('classification_report \\n',classification_report(y_test, y_pred_new))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1781/1781 [==============================] - 2s 990us/step\n",
            "accuracy_score: \n",
            " 0.9911695516309118\n",
            "confusion_matrix \n",
            " [[56399   465]\n",
            " [   38    60]]\n",
            "classification_report \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00     56864\n",
            "           1       0.11      0.61      0.19        98\n",
            "\n",
            "    accuracy                           0.99     56962\n",
            "   macro avg       0.56      0.80      0.59     56962\n",
            "weighted avg       1.00      0.99      0.99     56962\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAD-ygwbwklh"
      },
      "source": [
        "Model eight\r\n",
        "\r\n",
        "Two layer sparse overcomplete autoencoder with linear activation and dropout\r\n",
        "29 -> 40 -> 29\r\n",
        "Dropout percentage: 5%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjnDenovg_RM",
        "outputId": "f72881cf-c75b-4747-9c6b-6f5cfb1b834c"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(units=40, activation='linear',  \\\n",
        "    activity_regularizer=regularizers.l1(10e-5), input_dim=29))\n",
        "model.add(Dropout(0.05))\n",
        "\n",
        "# Generate output layer with 29 nodes\n",
        "model.add(Dense(units=29, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(x=X_train, y=X_train,\n",
        "                    epochs=num_epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True,\n",
        "                    validation_data=(X_train, X_train),\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "7121/7121 [==============================] - 18s 3ms/step - loss: 0.2198 - accuracy: 0.7181 - val_loss: 0.0049 - val_accuracy: 0.9806\n",
            "Epoch 2/10\n",
            "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0404 - accuracy: 0.8610 - val_loss: 0.0047 - val_accuracy: 0.9822\n",
            "Epoch 3/10\n",
            "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0411 - accuracy: 0.8613 - val_loss: 0.0074 - val_accuracy: 0.9724\n",
            "Epoch 4/10\n",
            "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0415 - accuracy: 0.8601 - val_loss: 0.0050 - val_accuracy: 0.9789\n",
            "Epoch 5/10\n",
            "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0413 - accuracy: 0.8602 - val_loss: 0.0047 - val_accuracy: 0.9839\n",
            "Epoch 6/10\n",
            "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0409 - accuracy: 0.8622 - val_loss: 0.0049 - val_accuracy: 0.9769\n",
            "Epoch 7/10\n",
            "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0397 - accuracy: 0.8611 - val_loss: 0.0044 - val_accuracy: 0.9805\n",
            "Epoch 8/10\n",
            "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0419 - accuracy: 0.8623 - val_loss: 0.0039 - val_accuracy: 0.9830\n",
            "Epoch 9/10\n",
            "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0410 - accuracy: 0.8638 - val_loss: 0.0047 - val_accuracy: 0.9791\n",
            "Epoch 10/10\n",
            "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0427 - accuracy: 0.8625 - val_loss: 0.0060 - val_accuracy: 0.9780\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "O9TWW3lPg_TJ",
        "outputId": "e8ea30ff-7d10-41e9-c710-ed291e436ca6"
      },
      "source": [
        "# Evaluate on train set\n",
        "predictions = model.predict(X_train, verbose=1) # prediction is nearly the same as X_train\n",
        "predictions = pd.DataFrame(data=predictions,index=X_train.index)\n",
        "\n",
        "originalDF = X_train\n",
        "reducedDF = predictions\n",
        "loss = np.sum((np.array(originalDF) - \n",
        "                np.array(reducedDF))**2, axis=1)\n",
        "loss = pd.Series(data=loss,index=originalDF.index)\n",
        "loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
        "anomalyScores = loss\n",
        "anomalyScores_h2o = h2o.H2OFrame(pd.DataFrame(anomalyScores))\n",
        "anomalyScores_h2o.quantile() # ti le fraud\n",
        "anomalyScores_h2o.quantile(prob=[0.998271]) # ti le fraud"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7121/7121 [==============================] - 7s 958us/step\n",
            "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th style=\"text-align: right;\">   Probs</th><th style=\"text-align: right;\">  0Quantiles</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td style=\"text-align: right;\">0.998271</td><td style=\"text-align: right;\">   0.0556092</td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VVoSGNvw5sp",
        "outputId": "a9566e12-85c0-49be-e01b-78b44fad2f31"
      },
      "source": [
        "best_threshold=0.0556092\n",
        "y_pred_new = [1 if x >= best_threshold else 0 for x in anomalyScores]\n",
        "confmat = confusion_matrix(y_train,y_pred_new)\n",
        "print('accuracy_score: \\n',accuracy_score(y_train, y_pred_new))\n",
        "print('confusion_matrix \\n',confmat)\n",
        "print('classification_report \\n',classification_report(y_train, y_pred_new))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_score: \n",
            " 0.9986920933090478\n",
            "confusion_matrix \n",
            " [[227302    149]\n",
            " [   149    245]]\n",
            "classification_report \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    227451\n",
            "           1       0.62      0.62      0.62       394\n",
            "\n",
            "    accuracy                           1.00    227845\n",
            "   macro avg       0.81      0.81      0.81    227845\n",
            "weighted avg       1.00      1.00      1.00    227845\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxx1B_P5xE0Z",
        "outputId": "ba6f35b2-6d7b-43cf-a2ff-948ca313e180"
      },
      "source": [
        "# Evaluate on test set\n",
        "predictions = model.predict(X_test, verbose=1) # prediction is nearly the same as X_train\n",
        "predictions = pd.DataFrame(data=predictions,index=X_test.index)\n",
        "\n",
        "originalDF = X_test\n",
        "reducedDF = predictions\n",
        "loss = np.sum((np.array(originalDF) - \n",
        "                np.array(reducedDF))**2, axis=1)\n",
        "loss = pd.Series(data=loss,index=originalDF.index)\n",
        "loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
        "anomalyScores=loss\n",
        "best_threshold=0.0183468\n",
        "y_pred_new = [1 if x >= best_threshold else 0 for x in anomalyScores]\n",
        "confmat = confusion_matrix(y_test,y_pred_new)\n",
        "print('accuracy_score: \\n',accuracy_score(y_test, y_pred_new))\n",
        "print('confusion_matrix \\n',confmat)\n",
        "print('classification_report \\n',classification_report(y_test, y_pred_new))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1781/1781 [==============================] - 2s 925us/step\n",
            "accuracy_score: \n",
            " 0.9974368877497279\n",
            "confusion_matrix \n",
            " [[56738   126]\n",
            " [   20    78]]\n",
            "classification_report \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.38      0.80      0.52        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.69      0.90      0.76     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFIN43Dm_sj5"
      },
      "source": [
        "# ML method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be0txs_ITNut"
      },
      "source": [
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6fZ1xTO_uwt"
      },
      "source": [
        "file='https://media.githubusercontent.com/media/aapatel09/handson-unsupervised-learning/master/datasets/credit_card_data/credit_card.csv'\r\n",
        "data = pd.read_csv(file)\r\n",
        "\r\n",
        "dataX = data.copy().drop(['Class'],axis=1)\r\n",
        "dataY = data['Class'].copy()\r\n",
        "\r\n",
        "featuresToScale = dataX.columns\r\n",
        "sX =StandardScaler(copy=True)\r\n",
        "dataX.loc[:,featuresToScale] = sX.fit_transform(dataX[featuresToScale])\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = \\\r\n",
        "    train_test_split(dataX, dataY, test_size=0.20, \\\r\n",
        "                    random_state=2018, stratify=dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gFwhKaO_uzB",
        "outputId": "e322ab0d-68be-4c17-d99e-f2a1306caf11"
      },
      "source": [
        "start=time.time()\r\n",
        "xgb=XGBClassifier(\r\n",
        "                nthread=16,\r\n",
        "                objective='binary:logistic',\r\n",
        "                eval_metric = 'logloss',\r\n",
        "                #scale_pos_weight=1,\r\n",
        "                seed=2018,\r\n",
        "                verbosity=1    \r\n",
        "                              )\r\n",
        "xgb.fit(X_train, y_train)\r\n",
        "end=time.time() \r\n",
        "print(\"time: \",end-start)\r\n",
        "\r\n",
        "y_pred=xgb.predict(X_test)\r\n",
        "y_pred_probs=xgb.predict_proba(X_test)\r\n",
        "\r\n",
        "precision, recall, thresholds = precision_recall_curve(y_test,y_pred_probs[:,1])\r\n",
        "average_precision = average_precision_score(y_test,y_pred_probs[:,1])\r\n",
        "\r\n",
        "fpr, tpr, thresholds = roc_curve(y_test,y_pred_probs[:,1])\r\n",
        "areaUnderROC = auc(fpr, tpr)\r\n",
        "print('average_precision',average_precision)\r\n",
        "print('AUC',areaUnderROC)\r\n",
        "print(\"f1 score\",f1_score(y_test,y_pred))\r\n",
        "print(\"balanced_accuracy_score:\",balanced_accuracy_score(y_test,y_pred))\r\n",
        "print('classification_report \\n',classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time:  48.214619398117065\n",
            "average_precision 0.8600324926641665\n",
            "AUC 0.9786933449519368\n",
            "f1 score 0.8972972972972973\n",
            "balanced_accuracy_score: 0.9234342161175106\n",
            "classification_report \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.95      0.85      0.90        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.98      0.92      0.95     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0IoMflh_u1m",
        "outputId": "d98a1810-0748-4951-8eee-55c91bd61a6f"
      },
      "source": [
        "start=time.time()\r\n",
        "lgbm=LGBMClassifier(\r\n",
        "    n_estimators = 200,\r\n",
        "    boosting_type = 'gbdt',\r\n",
        "    objective = 'binary',\r\n",
        "    metric= 'binary:logloss',\r\n",
        "    metric_freq=50,\r\n",
        "    learning_rate=0.01,\r\n",
        "    max_depth=4,\r\n",
        "    num_leaves= 16, #(2^maxdepth)\r\n",
        "    #min_data_in_leaf=2000,\r\n",
        "    scale_pos_weight=1,\r\n",
        "    num_threads=16,\r\n",
        "    random_state =500,\r\n",
        "    bagging_freq=0,\r\n",
        "                                )\r\n",
        "lgbm.fit(X_train, y_train)\r\n",
        "end=time.time() \r\n",
        "print(\"time: \",end-start)\r\n",
        "\r\n",
        "y_pred=lgbm.predict(X_test)\r\n",
        "y_pred_probs=lgbm.predict_proba(X_test)\r\n",
        "\r\n",
        "precision, recall, thresholds = precision_recall_curve(y_test,y_pred_probs[:,1])\r\n",
        "average_precision = average_precision_score(y_test,y_pred_probs[:,1])\r\n",
        "\r\n",
        "fpr, tpr, thresholds = roc_curve(y_test,y_pred_probs[:,1])\r\n",
        "areaUnderROC = auc(fpr, tpr)\r\n",
        "print('average_precision',average_precision)\r\n",
        "print('AUC',areaUnderROC)\r\n",
        "print(\"f1 score\",f1_score(y_test,y_pred))\r\n",
        "print(\"balanced_accuracy_score:\",balanced_accuracy_score(y_test,y_pred))\r\n",
        "print('classification_report \\n',classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time:  8.180473566055298\n",
            "average_precision 0.8315971400187149\n",
            "AUC 0.9737933616046306\n",
            "f1 score 0.8491620111731844\n",
            "balanced_accuracy_score: 0.887711137493827\n",
            "classification_report \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.94      0.78      0.85        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.97      0.89      0.92     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}